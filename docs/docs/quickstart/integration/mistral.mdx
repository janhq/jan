---
title: Mistral AI
sidebar_position: 7
description: A step-by-step guide on how to integrate Jan with Mistral AI.
keywords:
  [
    Jan AI,
    Jan,
    ChatGPT alternative,
    local AI,
    private AI,
    conversational AI,
    no-subscription fee,
    large language model,
    Mistral integration,
  ]
---

import flow from './assets/mistral.png';

## How to Integrate Mistral AI with Jan

[Mistral AI](https://docs.mistral.ai/) provides two ways to use their Large Language Models (LLM): 
1. API
2. Open-source models on Hugging Face. 

<div class="text--center" > 
    <img src={ flow } width = { 800} alt = "Mistral" />
</div>

To integrate Jan with Mistral AI, follow the steps below:

:::note
This tutorial demonstrates integrating Mistral AI with Jan using the API.
:::

### Step 1: Configure Mistral API Key

1. Obtain Mistral API keys from your [Mistral](https://console.mistral.ai/user/api-keys/) dashboard.
2. Insert the Mistral AI API key into `~/jan/engines/openai.json`.

```json title="~/jan/engines/openai.json"
{
  "full_url": "https://api.mistral.ai/v1/chat/completions",
  "api_key": "<your-mistral-ai-api-key>"
}
```

### Step 2: Model Configuration

1. Navigate to `~/jan/models`.
2. Create a folder named `mistral-(modelname)` (e.g., `mistral-tiny`).
3. Inside, create a `model.json` file with these settings:
    - Set `id` to the Mistral AI model ID.
    - Set `format` to `api`.
    - Set `engine` to `openai`.
    - Set `state` to `ready`.

```json title="~/jan/models/mistral-tiny/model.json"
{
  "sources": [
    {
      "filename": "mistral-tiny",
      "url": "https://mistral.ai/"
    }
  ],
  "id": "mistral-tiny",
  "object": "model",
  "name": "Mistral-7B-v0.2 (Tiny Endpoint)",
  "version": "1.0",
  "description": "Currently powered by Mistral-7B-v0.2, a better fine-tuning of the initial Mistral-7B released, inspired by the fantastic work of the community.",
  "format": "api",
  "settings": {},
  "parameters": {},
  "metadata": {
    "author": "Mistral AI",
    "tags": ["General", "Big Context Length"]
  },
  "engine": "openai"
}

```
### Regarding `model.json`

- In `settings`, two crucial values are:
  - `ctx_len`: Defined based on the model's context size.
  - `prompt_template`: Defined based on the model's trained template (e.g., ChatML, Alpaca).
  - To set up the `prompt_template`:
    1. Visit [Hugging Face](https://huggingface.co/), an open-source machine learning platform.
    2. Find the current model that you're using (e.g., [Gemma 7b it](https://huggingface.co/google/gemma-7b-it)).
    3. Review the text and identify the template.
- In `parameters`, consider the following options. The fields in `parameters` are typically general and can be the same across models. An example is provided below:

```json
"parameters":{
  "temperature": 0.7,
  "top_p": 0.95,
  "stream": true,
  "max_tokens": 4096,
  "frequency_penalty": 0,
  "presence_penalty": 0
}
```

:::note

Mistral AI offers various endpoints. Refer to their [endpoint documentation](https://docs.mistral.ai/platform/endpoints/) to select the one that fits your requirements. Here, we use the `mistral-tiny` model as an example.

:::

### Step 3: Start the Model

1. Restart Jan and navigate to the **Hub**. 
2. Locate your model and click the **Use** button.