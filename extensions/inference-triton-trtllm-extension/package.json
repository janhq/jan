{
  "name": "@janhq/inference-triton-trt-llm-extension",
  "version": "1.0.0",
  "description": "This extension enables Nvidia's TensorRT-LLM as an inference engine option",
  "main": "dist/index.js",
  "module": "dist/module.js",
  "author": "Jan <service@jan.ai>",
  "license": "AGPL-3.0",
  "scripts": {
    "build": "tsc -b . && webpack --config webpack.config.js",
    "build:publish": "rimraf *.tgz --glob && npm run build && npm pack && cpx *.tgz ../../pre-install"
  },
  "exports": {
    ".": "./dist/index.js",
    "./main": "./dist/module.js"
  },
  "devDependencies": {
    "cpx": "^1.5.0",
    "rimraf": "^3.0.2",
    "webpack": "^5.88.2",
    "webpack-cli": "^5.1.4",
    "ts-loader": "^9.5.0"
  },
  "dependencies": {
    "@janhq/core": "file:../../core",
    "fetch-retry": "^5.0.6",
    "path-browserify": "^1.0.1",
    "ulid": "^2.3.0",
    "rxjs": "^7.8.1"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "files": [
    "dist/*",
    "package.json",
    "README.md"
  ],
  "bundleDependencies": [
    "fetch-retry"
  ]
}
