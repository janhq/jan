# docker version
version: "3"

services:
  keycloak:
    image: quay.io/keycloak/keycloak:${KEYCLOAK_VERSION-22.0.0}
    command: ["start-dev", "--import-realm", "--http-port", "8088"]
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: keycloak_postgres
      KC_DB_URL_DATABASE: ${POSTGRES_DB_NAME:-keycloak}
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      KC_DB_USERNAME: ${POSTGRES_USERNAME:-postgres}
      KC_DB_SCHEMA: ${KC_DB_SCHEMA:-public}
      KC_HEALTH_ENABLED: "true"
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD-admin}
    volumes:
      - ./conf/keycloak_conf:/opt/keycloak/data/import
    ports:
      - "8088:8088"
    depends_on:
      keycloak_postgres:
        condition: service_healthy
    networks:
      jan_community:
        ipv4_address: 172.20.0.9

  keycloak_postgres:
    image: postgres:15
    command: postgres -c 'max_connections=200' && postgres -c 'shared_buffers=24MB'
    environment:
      # Environment Variables expecially for Postgres
      POSTGRES_DB: ${POSTGRES_DB_NAME:-keycloak}
      POSTGRES_USER: ${POSTGRES_USERNAME:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      PGDATA: /data/postgres
      PGPORT: ${POSTGRES_PORT:-5432}
    healthcheck:
      test: "exit 0"

    ports:
      - ${POSTGRES_PORT:-5432}:${POSTGRES_PORT:-5432}
    networks:
      jan_community:
        ipv4_address: 172.20.0.10

  postgres:
    image: postgres:15
    restart: always

    env_file:
      - conf/sample.env_app-backend-postgres
    networks:
      jan_community:
        ipv4_address: 172.20.0.11

  graphql-engine:
    image: hasura/graphql-engine:v2.31.0.cli-migrations-v3
    ports:
      - "8080:8080"
    restart: always
    env_file:
      - conf/sample.env_app-backend
    volumes:
      - ./app-backend/hasura/migrations:/migrations
      - ./app-backend/hasura/metadata:/metadata
    depends_on:
      data-connector-agent:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 10
    networks:
      jan_community:
        ipv4_address: 172.20.0.12

  hasura-seed-apply:
    image: hasura/graphql-engine:v2.31.0.cli-migrations-v3
    entrypoint: [""]
    command: ["/bin/sh", "-c", "hasura-cli seed apply --all-databases"]
    env_file:
      - conf/sample.env_app-backend
    volumes:
      - ./app-backend/hasura/config.yaml:/config.yaml
      - ./app-backend/hasura/seeds:/seeds
    depends_on:
      graphql-engine:
        condition: service_healthy
    extra_hosts:
      - "localhost:172.20.0.12"
    networks:
      jan_community:

  worker:
    build:
      context: ./app-backend/worker
      dockerfile: ./Dockerfile
    restart: always
    environment:
      - "NODE_ENV=development"
    volumes:
      - ./app-backend/worker:/worker
    ports:
      - "8787:8787"
    networks:
      jan_community:
        ipv4_address: 172.20.0.13

  data-connector-agent:
    image: hasura/graphql-data-connector:v2.31.0
    restart: always
    ports:
      - 8081:8081
    environment:
      QUARKUS_LOG_LEVEL: ERROR # FATAL, ERROR, WARN, INFO, DEBUG, TRACE
      ## https://quarkus.io/guides/opentelemetry#configuration-reference
      QUARKUS_OPENTELEMETRY_ENABLED: "false"
      ## QUARKUS_OPENTELEMETRY_TRACER_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/api/v1/athena/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 5s
    networks:
      jan_community:
        ipv4_address: 172.20.0.14

  web:
    build:
      context: ./web-client
      dockerfile: ./dev.Dockerfile
    restart: always
    volumes:
      - ./web-client/:/app
      - /app/node_modules
      - /app/.next
    env_file:
      - conf/sample.env_web-client
    ports:
      - "3000:3000"
    environment:
      NODE_ENV: development
    extra_hosts:
      - "localhost:172.20.0.9"
    networks:
      jan_community:
        ipv4_address: 172.20.0.15

  # Service to run the Llama web application.
  llm:
    image: ghcr.io/abetlen/llama-cpp-python@sha256:b6d21ff8c4d9baad65e1fa741a0f8c898d68735fff3f3cd777e3f0c6a1839dd4
    # Mount the directory that contains the downloaded model.
    volumes:
      - ./jan-inference/llm/models:/models
    ports:
      - 8000:8000
    environment:
      # Specify the path to the model for the web application.
      MODEL: /models/${LLM_MODEL_FILE}
      PYTHONUNBUFFERED: 1
    # Restart policy configuration
    restart: on-failure
    # Specifies that this service should start only after wait-for-downloader has completed successfully.
    networks:
      jan_community:
        ipv4_address: 172.20.0.18

  sd-downloader:
    build:
      context: ./jan-inference/sd/
      dockerfile: compile.Dockerfile
    # The command extracts the model filename from MODEL_URL and downloads it if it doesn't exist.
    command: /bin/sh -c "if [ ! -f /models/*.bin ]; then python /sd.cpp/sd_cpp/models/convert.py  --out_type q4_0 --out_file /models/${SD_MODEL_FILE}.q4_0.bin  /models/${SD_MODEL_FILE}; fi"
    # Mount a local directory to store the downloaded model.
    volumes:
      - ./jan-inference/sd/models:/models
    networks:
      jan_community:
        ipv4_address: 172.20.0.19

  # Service to run the SD web application.
  sd:
    build:
      context: ./jan-inference/sd/
      dockerfile: inference.Dockerfile
    # Mount the directory that contains the downloaded model.
    volumes:
      - ./jan-inference/sd/models:/models/
    command: /bin/bash -c "python -m uvicorn main:app --proxy-headers --host 0.0.0.0 --port 8000"
    environment:
      # Specify the path to the model for the web application.
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      S3_PUBLIC_ENDPOINT_URL: ${S3_PUBLIC_ENDPOINT_URL}
      S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY}
      S3_BUCKET_NAME: ${S3_BUCKET_NAME}
      MODEL_NAME: ${SD_MODEL_FILE}.q4_0.bin
      MODEL_DIR: /models
      OUTPUT_DIR: /tmp
      SD_PATH: /sd
      PYTHONUNBUFFERED: 1
    ports:
      - 8001:8000
    # Restart policy configuration
    restart: on-failure
    # Specifies that this service should start only after wait-for-downloader has completed successfully.
    depends_on:
      sd-downloader:
        condition: service_completed_successfully
    networks:
      jan_community:
        ipv4_address: 172.20.0.21

  minio:
    image: minio/minio
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - ./minio/data:/export
      - ./minio/config:/root/.minio
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY_ID}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_ACCESS_KEY}
    command: server /export --console-address ":9001"
    networks:
      jan_community:
        ipv4_address: 172.20.0.23

  createbuckets:
    image: minio/mc
    depends_on:
      - minio
    environment:
      S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY}
      S3_BUCKET_NAME: ${S3_BUCKET_NAME}
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc config host add myminio http://minio:9000 ${S3_ACCESS_KEY_ID} ${S3_SECRET_ACCESS_KEY};
      /usr/bin/mc rm -r --force myminio/${S3_BUCKET_NAME};
      /usr/bin/mc mb myminio/${S3_BUCKET_NAME};
      /usr/bin/mc anonymous set public myminio/${S3_BUCKET_NAME};
      exit 0;
      "
    networks:
      jan_community:

  # Service for Traefik, a modern HTTP reverse proxy and load balancer.
  # traefik:
  #   image: traefik:v2.10
  #   ports:
  #     # Map port 80 in the container to port 80 on the host.
  #     - "80:80"
  #     # Map port 8080 in the container (Traefik's dashboard) to port 8080 on the host.
  #     - "8080:8080"
  #   # Mount the Docker socket to allow Traefik to listen to Docker's API.
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #     - ./traefik/traefik.yml:/traefik.yml:ro
  #     - ./traefik/config.yml:/config.yml:ro
  #   networks:
  #     jan_community:
  #       ipv4_address: 172.20.0.22

networks:
  jan_community:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
