[
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-conversational-extension-1.0.0.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/conversational-extension",
    "productName": "Conversational",
    "version": "1.0.0",
    "main": "dist/index.js",
    "description": "This extension enables conversations and state persistence via your filesystem",
    "url": "../../extensions/conversational-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-model-extension-1.0.35.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/model-extension",
    "productName": "Model Management",
    "version": "1.0.35",
    "main": "dist/index.js",
    "description": "Model Management Extension provides model exploration and seamless downloads",
    "url": "../../extensions/model-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-anthropic-extension-1.0.3.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-anthropic-extension",
    "productName": "Anthropic Inference Engine",
    "version": "1.0.3",
    "main": "dist/index.js",
    "description": "This extension enables Anthropic chat completion API calls",
    "url": "../../extensions/inference-anthropic-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-martian-extension-1.0.1.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-martian-extension",
    "productName": "Martian Inference Engine",
    "version": "1.0.1",
    "main": "dist/index.js",
    "description": "This extension enables Martian chat completion API calls",
    "url": "../../extensions/inference-martian-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-nvidia-extension-1.0.1.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-nvidia-extension",
    "productName": "NVIDIA NIM Inference Engine",
    "version": "1.0.1",
    "main": "dist/index.js",
    "description": "This extension enables NVIDIA chat completion API calls",
    "url": "../../extensions/inference-nvidia-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-openrouter-extension-1.0.0.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-openrouter-extension",
    "productName": "OpenRouter Inference Engine",
    "version": "1.0.0",
    "main": "dist/index.js",
    "description": "This extension enables Open Router chat completion API calls",
    "url": "../../extensions/inference-openrouter-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-cohere-extension-1.0.0.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-cohere-extension",
    "productName": "Cohere Inference Engine",
    "version": "1.0.0",
    "main": "dist/index.js",
    "description": "This extension enables Cohere chat completion API calls",
    "url": "../../extensions/inference-cohere-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-groq-extension-1.0.1.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-groq-extension",
    "productName": "Groq Inference Engine",
    "version": "1.0.1",
    "main": "dist/index.js",
    "description": "This extension enables fast Groq chat completion API calls",
    "url": "../../extensions/inference-groq-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-openai-extension-1.0.5.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-openai-extension",
    "productName": "OpenAI Inference Engine",
    "version": "1.0.5",
    "main": "dist/index.js",
    "description": "This extension enables OpenAI chat completion API calls",
    "url": "../../extensions/inference-openai-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-mistral-extension-1.0.1.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-mistral-extension",
    "productName": "MistralAI Inference Engine",
    "version": "1.0.1",
    "main": "dist/index.js",
    "description": "This extension enables Mistral chat completion API calls",
    "url": "../../extensions/inference-mistral-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-triton-trt-llm-extension-1.0.0.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-triton-trt-llm-extension",
    "productName": "Triton-TRT-LLM Inference Engine",
    "version": "1.0.0",
    "main": "dist/index.js",
    "description": "This extension enables Nvidia's TensorRT-LLM as an inference engine option",
    "url": "../../extensions/inference-triton-trtllm-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-monitoring-extension-1.0.10.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/monitoring-extension",
    "productName": "System Monitoring",
    "version": "1.0.10",
    "main": "dist/index.js",
    "description": "This extension provides system health and OS level data",
    "url": "../../extensions/monitoring-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-assistant-extension-1.0.1.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/assistant-extension",
    "productName": "Jan Assistant",
    "version": "1.0.1",
    "main": "dist/index.js",
    "description": "This extension enables assistants, including Jan, a default assistant that can call all downloaded models",
    "url": "../../extensions/assistant-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-tensorrt-llm-extension-0.0.3.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/tensorrt-llm-extension",
    "productName": "TensorRT-LLM Inference Engine",
    "version": "0.0.3",
    "main": "dist/index.js",
    "description": "This extension enables Nvidia's TensorRT-LLM for the fastest GPU acceleration. See the [setup guide](https://jan.ai/guides/providers/tensorrt-llm/) for next steps.",
    "url": "../../extensions/tensorrt-llm-extension/dist/index.js"
  },
  {
    "_active": true,
    "listeners": {},
    "origin": "janhq-inference-cortex-extension-1.0.24.tgz",
    "installOptions": { "version": false, "fullMetadata": true },
    "name": "@janhq/inference-cortex-extension",
    "productName": "Cortex Inference Engine",
    "version": "1.0.24",
    "main": "dist/index.js",
    "description": "This extension embeds cortex.cpp, a lightweight inference engine written in C++. See https://jan.ai.\nAdditional dependencies could be installed to run without Cuda Toolkit installation.",
    "url": "../../extensions/inference-cortex-extension/dist/index.js"
  }
]
