---
title: Jan Server
description: Your own private AI cloud
sidebar:
  order: 4
---

import { Aside, Card, CardGrid } from '@astrojs/starlight/components';

Jan Server is Jan Desktop with multi-user support. Deploy it on your hardware to create your own private AI cloud for your team or organization.

<Aside type="note">
Coming soon. Join the early access list at [jan.ai/server](https://jan.ai/server).
</Aside>

## What is Jan Server?

```
Jan Server = Jan Desktop + Multi-user support + Real hardware
```

It's the same engine that powers Jan Desktop, scaled up for teams. Your data stays on your servers, your models run on your GPUs, your AI remains yours.

## Why Organizations Need This

### The Problem
Every API call to ChatGPT or Claude is:
- Your intellectual property leaving your network
- Potential training data for someone else's model
- A compliance nightmare waiting to happen
- A monthly bill that never ends

### The Solution
Jan Server gives you:
- **Complete control**: Your hardware, your rules
- **Total privacy**: Nothing leaves your network
- **Predictable costs**: One-time hardware investment
- **Compliance ready**: GDPR, HIPAA, SOC2 friendly

## Deployment Options

<CardGrid>
  <Card title="Small Team (5-10 users)" icon="users">
    **Hardware**: Single RTX 6000 Ada (48GB)
    **RAM**: 128GB
    **Models**: Up to 70B parameters
    **Cost**: ~$15k one-time
  </Card>

  <Card title="Department (10-50 users)" icon="building">
    **Hardware**: 2-4 GPU nodes
    **RAM**: 256GB per node
    **Models**: Multiple concurrent
    **Cost**: ~$50-100k one-time
  </Card>

  <Card title="Enterprise (50+ users)" icon="corporate">
    **Hardware**: DGX cluster
    **RAM**: As needed
    **Models**: Full range
    **Cost**: Custom quote
  </Card>
</CardGrid>

## Simple Deployment

### Docker (Recommended)
```yaml
version: '3'
services:
  jan-server:
    image: jan.ai/server:latest
    ports:
      - "80:80"
      - "1337:1337"
    volumes:
      - ./models:/models
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

### Kubernetes
For larger deployments with auto-scaling and high availability.

### Bare Metal
For maximum performance and custom configurations.

## Key Features

### Multi-User Management
- Individual accounts and API keys
- Usage tracking and quotas
- Model access controls
- Team collaboration

### Same API as Desktop
```python
# Your code doesn't change
client = OpenAI(
    base_url="https://jan.company.internal/v1",
    api_key="user-specific-key"
)
```

### Model Governance
- Control which models are available
- Set user permissions
- Monitor usage
- Ensure compliance

<Aside type="tip">
Jan Server uses the same models as Desktop. No special "enterprise" versions with inflated prices.
</Aside>

## Real Deployments

| Use Case | Setup | Result |
|----------|-------|---------|
| Law Firm | 2x RTX 6000, 200 users | Client data never leaves network |
| Hospital | DGX node, 500 users | HIPAA compliant AI assistant |
| Tech Startup | 4x RTX 4090, 50 users | 90% cost reduction vs. OpenAI |
| University | Multi-node cluster | Unrestricted research |

## Hardware Guide

### Minimum Requirements
- **GPU**: RTX 3090 or better (24GB VRAM)
- **CPU**: 16+ cores
- **RAM**: 64GB minimum
- **Storage**: 1TB NVMe SSD

### Recommended Setup
- **GPU**: RTX 6000 Ada or A100
- **CPU**: Dual socket Xeon/EPYC
- **RAM**: 128-256GB
- **Storage**: RAID NVMe array

### Scaling Considerations
- 1 GPU can serve ~5-10 concurrent users
- 70B models need 40-80GB VRAM
- CPU inference possible for smaller models
- Network: 10Gbps recommended

## Why Self-Host?

### For IT Teams
- No data leaves your network
- Complete audit trails
- Integrate with existing auth (LDAP/AD)
- Predictable resource usage

### For Security Teams
- Air-gapped deployment options
- End-to-end encryption
- No third-party access
- Full compliance control

### For Finance Teams
- One-time hardware cost
- No per-token pricing
- Predictable TCO
- Use existing infrastructure

## Coming Features

### Phase 1 (Launch)
- Basic multi-user support
- Web interface
- API compatibility
- Usage monitoring

### Phase 2 (Post-Launch)
- Advanced governance
- Fine-tuning interface
- Automated scaling
- Backup/restore

### Phase 3 (Future)
- Federated deployments
- Cross-region sync
- Advanced analytics
- Custom model training

<Aside type="caution">
Jan Server requires proper cooling and power for GPU hardware. Plan your infrastructure accordingly.
</Aside>

## Migration Path

### From Cloud AI
1. Deploy Jan Server
2. Import your workflows
3. Update API endpoints
4. Migrate users gradually

### From Jan Desktop
1. Same models work instantly
2. Add user management
3. Scale as needed

## The Philosophy

We believe organizations should own their AI infrastructure just like they own their data. Jan Server makes this possible without compromising on capabilities.

This isn't about avoiding the cloud - it's about having a choice. Run your AI where it makes sense for your organization.

## Support Options

### Community Edition
- Full features
- Community support
- Perfect for small teams

### Enterprise Edition
- Priority support
- Custom deployment help
- SLA guarantees
- Training included

## Get Started

Jan Server is coming soon. While you wait:

1. **Plan your hardware**: Check our requirements above
2. **Join early access**: Get notified when available
3. **Test with Desktop**: Same models, same experience
4. **Prepare your team**: AI that respects your infrastructure

---

[Join Early Access](https://jan.ai/server) | [Hardware Guide](https://jan.ai/docs/server/hardware) | [Enterprise Contact](https://jan.ai/enterprise)
